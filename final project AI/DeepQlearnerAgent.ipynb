{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepQlearnerAgent.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DjrFc9u9aBep"},"outputs":[],"source":["import numpy as np\n","from keras import Sequential, Model\n","from keras.layers import Dense, Embedding, Reshape, Activation, Conv2D, BatchNormalization, Flatten, Dropout, Input\n","from keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Adam\n","\n","from Agent import Agent\n","from CreateBoard import *\n","from MinMaxAgent import MinMaxAgent\n","from Utils import *\n","\n","import random\n","\n","from RandomAgent import RandomAgent\n","\n","OFFSET = 10\n","\n","\n","class DeepQLearnerAgent(Agent):\n","\n","    def __init__(self, board, heuristic, depth, current_player):\n","        # TODO: check values of parameters\n","        super().__init__(board, heuristic, depth, current_player)\n","        self.epsilon = 0.3\n","        self.decay_epsilon = 0.995\n","        self.learning_rate = 0.000025\n","        self.gamma = 0.3\n","        self.model = None\n","        self.current_player = current_player\n","        self.build_model()\n","\n","    def build_model(self):\n","        main_input = Input((1,8,8))\n","        flattened = Flatten(name='flatten')(main_input)\n","        dense = Dense(256, name='dense1')(flattened)\n","        d1 = Dense(128, activation='relu6', name='dense2')(dense)\n","        d3 = Dense(64, activation='linear', name='dense4')(d1)\n","        d4 = Dense(32, activation='linear', name='dense5')(d3)\n","        d5 = Dense(16, activation='linear', name='dense6')(d4)\n","        d2 = Dense(1, name='dense3')(d5)\n","\n","        self.model = keras.models.Model(inputs=main_input, outputs=d2)\n","\n","        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n","    # self.input_boards = Input(shape=(WIDTH, HEIGHT))\n","        # x_image = Reshape((WIDTH, HEIGHT, 1))(self.input_boards)  # batch_size  x board_x x board_y x 1\n","        # h_conv1 = Activation('relu')(BatchNormalization(axis=3)(\n","        #     Conv2D(15, 3, padding='same', use_bias=False)(\n","        #         x_image)))  # batch_size  x board_x x board_y x num_channels\n","        # h_conv2 = Activation('relu')(BatchNormalization(axis=3)(\n","        #     Conv2D(15, 3, padding='same', use_bias=False)(\n","        #         h_conv1)))  # batch_size  x board_x x board_y x num_channels\n","        # h_conv3 = Activation('relu')(BatchNormalization(axis=3)(\n","        #     Conv2D(15, 3, padding='valid', use_bias=False)(\n","        #         h_conv2)))  # batch_size  x (board_x-2) x (board_y-2) x num_channels\n","        # h_conv4 = Activation('relu')(BatchNormalization(axis=3)(\n","        #     Conv2D(15, 3, padding='valid', use_bias=False)(\n","        #         h_conv3)))  # batch_size  x (board_x-4) x (board_y-4) x num_channels\n","        # h_conv4_flat = Flatten()(h_conv4)\n","        # s_fc1 = Dropout(0.5)(Activation('relu')(\n","        #     BatchNormalization(axis=1)(Dense(1024, use_bias=False)(h_conv4_flat))))  # batch_size x 1024\n","        # s_fc2 = Dropout(0.5)(\n","        #     Activation('relu')(BatchNormalization(axis=1)(Dense(512, use_bias=False)(s_fc1))))  # batch_size x 1024\n","        # self.pi = Dense(65, activation='softmax', name='pi')(s_fc2)  # batch_size x self.action_size\n","        # self.v = Dense(1, activation='tanh', name='v')(s_fc2)  # batch_size x 1\n","        #\n","        # self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n","        # self.model.compile(Adam(self.learning_rate), \"mse\")\n","\n","    def calculate_q_value(self, board, player):\n","        to_binary = np.zeros((8, 8))\n","        for i in range(WIDTH):\n","            for j in range(HEIGHT):\n","                if board[i][j].player == Player.PLAYER_2:\n","                    to_binary[i][j] = 2.0\n","                elif board[i][j].player == Player.PLAYER_1:\n","                    to_binary[i][j] = 1.0\n","        # to_binary = np.array([to_binary]).astype(np.float)\n","        to_binary = np.array([to_binary])\n","        q_max = self.model.predict(np.array([to_binary]))\n","        q_func = self.heuristic(board, player) + self.gamma * q_max\n","        return q_func, to_binary\n","\n","\n","\n","    def train(self, agent1, agent2, numGames, path):\n","        losses = []\n","        player_agent_dict = {Player.PLAYER_1.name: agent1, Player.PLAYER_2.name: agent2}\n","        for i in range(numGames):\n","            q_score = []\n","            boards = []\n","            curr_board = CREATE_BOARD_TYPE()\n","            curr_board.screen_update()\n","            while True:\n","                if curr_board.won:\n","                    break\n","                rand_float = np.random.rand()\n","                player_agent_dict[curr_board.player.name].board = curr_board\n","                if rand_float >= self.epsilon:\n","\n","                    act, _ = player_agent_dict[curr_board.player.name].do_action()\n","                else:\n","\n","                    act, _ = RandomAgent(curr_board, None, None, None).do_action()\n","                if act is not None:\n","                    x, y = act.loc\n","                    curr_board.move(x, y)\n","                    if curr_board.player == Player.PLAYER_1:\n","                        q_func, to_binary = self.calculate_q_value(curr_board.board, curr_board.player)\n","                        q_score.append(q_func)\n","                        boards.append(to_binary)\n","                else:\n","                    break\n","            q_score2 = np.array(q_score)\n","            boards2 = np.array(boards)\n","            loss = self.model.fit(boards2, q_score2, epochs = EPOCHS)\n","            losses.append(loss)\n","            self.epsilon = self.epsilon * self.decay_epsilon\n","        self.model.save_weights(path)\n","        return losses\n","\n","# player0 = RandomAgent(None, None, None, 0)\n","# player1 = RandomAgent(None, None, None, 1)\n","# d = DeepQLearnerAgent(None, weight_place_heuristic, None, 0)\n","# d.train(player0, player1, 1, \"player0_minmax_player1_random\")\n"]}]}
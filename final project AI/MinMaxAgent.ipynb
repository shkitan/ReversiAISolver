{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MinMaxAgent.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOacJmgETc7tszXNlPSYSjF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yUHJICTnabPl"},"outputs":[],"source":["import numpy as np\n","from Agent import Agent\n","from CreateBoard import *\n","from Utils import *\n","dict_idx_to_player = { 0: Player.PLAYER_1, 1: Player.PLAYER_2}\n","\n","class MinMaxAgent(Agent):\n","    def __init__(self, board, heuristic, depth, current_player):\n","        super().__init__(board, heuristic, depth, current_player)\n","\n","#### Original\n","\n","\n","    def alpha_beta_algorithm(self, alpha, beta, state, index_player, depth):\n","        if depth == 0:\n","            return self.heuristic(state, dict_idx_to_player[index_player])\n","        if not self.has_legal_action():\n","            return self.heuristic(state, dict_idx_to_player[index_player])\n","        if index_player == 1:  # Min player\n","            actions = CREATE_BOARD_TYPE.static_legal_moves(state, dict_idx_to_player[index_player])\n","            num_actions = len(actions)\n","            action_idx = 0\n","            while num_actions > 0:\n","                beta = min(beta, self.alpha_beta_algorithm(alpha, beta, actions[action_idx].board, 1 - index_player, depth - 1))\n","                if alpha >= beta:\n","                    break\n","                num_actions -= 1\n","                action_idx += 1\n","            return beta\n","        else:  # Max player\n","            actions = CREATE_BOARD_TYPE.static_legal_moves(state, dict_idx_to_player[index_player])\n","            num_actions = len(actions)\n","            action_idx = 0\n","            while num_actions > 0:\n","                alpha = max(alpha, self.alpha_beta_algorithm(alpha, beta, actions[action_idx].board, 1 - index_player, depth))\n","                if alpha >= beta:\n","                    break\n","                num_actions -= 1\n","                action_idx += 1\n","            return alpha\n","\n","    def do_action(self):\n","        \"\"\"\n","        Returns the minimax action using self.depth and self.evaluationFunction\n","        \"\"\"\n","        actions = CREATE_BOARD_TYPE.static_legal_moves(self.board.board, self.board.player)\n","        num_actions = len(actions)\n","        action_idx = 0\n","        max_val = 0\n","        action = None\n","        alpha = -np.inf\n","        beta = np.inf\n","        while num_actions > 0:\n","            if action_idx == 0:\n","                val = self.alpha_beta_algorithm(alpha, beta,  actions[action_idx].board, 1, self.depth)\n","                max_val = val\n","                action = actions[action_idx]\n","            else:\n","                val = self.alpha_beta_algorithm(alpha, beta,  actions[action_idx].board, 1, self.depth)\n","                if val > max_val:\n","                    max_val = val\n","                    action = actions[action_idx]\n","            num_actions -= 1\n","            action_idx += 1\n","        return action, max_val\n","\n","\n","# New Shir and David - not good!!!\n","#     def alpha_beta_algorithm(self, alpha, beta, state, index_player, depth, a):\n","#         actions = CREATE_BOARD_TYPE.static_legal_moves(state,  dict_idx_to_player[index_player])\n","#\n","#         if depth == self.depth:\n","#             return a, self.heuristic(state, dict_idx_to_player[index_player])\n","#         if not self.has_legal_action():\n","#             return a, self.heuristic(state, dict_idx_to_player[index_player])\n","#         if dict_idx_to_player[index_player] != self.current_player:  # Min player\n","#             min_val = np.inf\n","#             act = None\n","#             num_actions = len(actions)\n","#             action_idx = 0\n","#             while num_actions > 0:\n","#                 _, score = self.alpha_beta_algorithm(alpha, beta, actions[action_idx].board, 1 - index_player, depth + 1, actions[action_idx])\n","#                 if score <= min_val:\n","#                     min_val = beta\n","#                     act = actions[action_idx]\n","#                 beta = min(beta, score)\n","#                 if alpha >= beta:\n","#                     break\n","#                 num_actions -= 1\n","#                 action_idx += 1\n","#             return act, min_val\n","#         else:  # Max player\n","#             max_val = -np.inf\n","#             act = None\n","#             num_actions = len(actions)\n","#             action_idx = 0\n","#             while num_actions > 0:\n","#                 _, score = self.alpha_beta_algorithm(alpha, beta, actions[action_idx].board, 1 - index_player, depth, actions[action_idx])\n","#                 if score >= max_val:\n","#                     max_val = score\n","#                     act = actions[action_idx]\n","#                     alpha = max(alpha, score)\n","#                 if alpha >= beta:\n","#                     break\n","#                 num_actions -= 1\n","#                 action_idx += 1\n","#             return act, max_val\n","#\n","#     def do_action(self):\n","#         \"\"\"\n","#         Returns the minimax action using self.depth and self.evaluationFunction\n","#         \"\"\"\n","#         action = None\n","#         alpha = -np.inf\n","#         beta = np.inf\n","#         action, max_val = self.alpha_beta_algorithm(alpha, beta,  self.board.board, self.current_player, 0, action)\n","#         return action, max_val\n","\n","\n","\n","\n","\n"]}]}